---
title: "Machine Learning Methods"
subtitle: "Regression, Classification, Topic Insights"
author:
  - name: Dakota Alder
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
  - name: Julio Garcia
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
---

```{python}
from pyspark.sql import SparkSession

# Start a Spark session
spark = SparkSession.builder.appName("JobPostingsAnalysis").getOrCreate()

# Load the CSV file into a Spark DataFrame
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("../data/lightcast_job_postings.csv")



```

```{python}
import pandas as pd
from pyspark.sql.functions import when, col

#Clean Data to convert to Pandas
columns_to_drop = ["ID", "BODY", "URL", "ACTIVE_URLS", "DUPLICATES", "LAST_UPDATED_TIMESTAMP",
    "NAICS2", "NAICS3", "NAICS4", "NAICS5", "NAICS6",
    "SOC_2", "SOC_3", "SOC_4", "SOC_5", "LAST_UPDATED_DATE", "LAST_UPDATED_TIMESTAMP", "EXPIRED", "SOURCE_TYPES", "SOURCES", "ACTIVE_SOURCES_INFO", "MODELED_EXPIRED", "MODELED_DURATION", "NAICS2_NAME", "NAICS3_NAME", "NAICS4_NAME", "NAICS5_NAME", "NAICS6_NAME",
    "SOC_2_NAME", "SOC_3_NAME", "SOC_4_NAME", "SOC_5_NAME", "EDUCATION_LEVELS", "MIN_EDULEVELS"
    
]
cleaned_data = df.drop(*columns_to_drop)

cleaned_data = cleaned_data.withColumn(
    "REMOTE_TYPE_NAME",
    when(col("REMOTE_TYPE_NAME") == "Remote", "Remote")
    .when(col("REMOTE_TYPE_NAME") == "Hybrid Remote", "Hybrid")
    .when(col("REMOTE_TYPE_NAME") == "[None]", "On-site")
    .when(col("REMOTE_TYPE_NAME").isNull(), "On-site")
    .when(col("REMOTE_TYPE_NAME") == "Not Remote", "On-site")
    .otherwise(col("REMOTE_TYPE_NAME"))
)

cleaned_data = cleaned_data.withColumn(
    "EMPLOYMENT_TYPE_NAME",
    when(col("EMPLOYMENT_TYPE_NAME") == "Part-time / full-time", "Flexible")
    .when(col("EMPLOYMENT_TYPE_NAME").isNull(), "Full-Time")
    .when(col("EMPLOYMENT_TYPE_NAME") == "Part-time (â‰¤ 32 hours)", "Part-Time")
    .when(col("EMPLOYMENT_TYPE_NAME") == "Full-time (> 32 hours)", "Full-Time")
    .otherwise(col("EMPLOYMENT_TYPE_NAME")) 
)

cleaned_data = cleaned_data.filter(col("NAICS_2022_2_NAME") != "Unclassified Industry")

median_salary = cleaned_data.approxQuantile("SALARY", [0.5], 0.01)[0]
cleaned_data = cleaned_data.withColumn(
    "SALARY",
    when(col("SALARY").isNull(), median_salary).otherwise(col("SALARY"))
)

#Convert to Pandas
clean_pdf = cleaned_data.toPandas()


```

```{python}
#| echo: true
#| eval: true
# Cleaning empty rows and dropping columns that are mostly empty


fill_cols = ["CITY_NAME", "CITY", "LOCATION", "STATE", "STATE_NAME", "COMPANY", "COMPANY_NAME"]
clean_pdf[fill_cols] = clean_pdf[fill_cols].fillna("Unknown")

clean_pdf = clean_pdf.drop_duplicates(subset=["TITLE", "COMPANY", "LOCATION", "POSTED"], keep="first")

clean_pdf.dropna(thresh=len(clean_pdf)*0.5, axis=1, inplace=True)

#New Column to Classify AI Jobs and Add Month of Posting Date

ai_keywords = [
    "AI", "Machine Learning", "Data Scientist", "Data Analyst", "ML", 
    "Artificial Intelligence", "Deep Learning", "NLP", "Predictive Analytics"
]

#Function to classify AI vs Non-AI Jobs
def classify_ai(title):
    title_lower = str(title).lower()
    for keyword in ai_keywords:
        if keyword.lower() in title_lower:
            return "AI"
    return "Non-AI"

clean_pdf["AI_JOB"] = clean_pdf["TITLE_RAW"].apply(classify_ai)

clean_pdf["POSTED"] = pd.to_datetime(clean_pdf["POSTED"], errors="coerce")
clean_pdf["POSTED_MONTH"] = clean_pdf["POSTED"].dt.month

#Add column for URBAN vs RURAL

clean_pdf["URBAN_RURAL"] = clean_pdf["MSA_NAME"].apply(lambda x: "Urban" if pd.notnull(x) else "Rural")

```

```{python}
#KMeans clustering using NAICS as a reference but not a target

from sklearn.preprocessing import StandardScaler

# Select features
features = clean_pdf[["SALARY", "AI_JOB", "REMOTE_TYPE_NAME", "URBAN_RURAL"]]

# One-hot encode categorical columns
features_encoded = pd.get_dummies(features, columns=["AI_JOB", "REMOTE_TYPE_NAME", "URBAN_RURAL"], drop_first=True)

# Standardize numerical features (important for KMeans)
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_encoded)

from sklearn.cluster import KMeans

k = 4
kmeans = KMeans(n_clusters=k, random_state=42)
clean_pdf["CLUSTER"] = kmeans.fit_predict(features_scaled)

#Use Industry Name (NAICS2022) as a reference label

cluster_summary = (
    clean_pdf.groupby(["CLUSTER", "NAICS_2022_2_NAME"])
    .size()
    .reset_index(name="count")
    .sort_values(["CLUSTER", "count"], ascending=[True, False])
)



#Used an Elbow Method to choose the correct number of clusters

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Example features
X = features_encoded.values  # your numerical features

wcss = []
for k in range(1, 15):
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(X)
    wcss.append(km.inertia_)

plt.plot(range(1, 15), wcss, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('WCSS (Inertia)')
plt.title('Elbow Method')
plt.show()

cluster_summary.head(20)  # Show top 20 to see patterns

one_hot_cols = ['AI_JOB_Non-AI', 'REMOTE_TYPE_NAME_On-site', 'REMOTE_TYPE_NAME_Remote', 'URBAN_RURAL_Urban']

clean_pdf = pd.concat([clean_pdf, features_encoded[one_hot_cols]], axis=1)


import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
sns.scatterplot(
    data=clean_pdf,
    x='SALARY',
    y='AI_JOB_Non-AI',  
    hue='CLUSTER',
    palette='viridis',
    alpha=0.7
)
plt.title("KMeans Clustering: Salary vs AI Jobs")
plt.xlabel("Salary")
plt.ylabel("AI JOB (1=Non-AI, 0=AI)")
plt.legend(title="Cluster")
plt.tight_layout()
plt.show()


```

```{python}
#Visualizing the NAICS Industries in reference to each cluster



import matplotlib.pyplot as plt


top_industries = (
    clean_pdf.groupby(["CLUSTER", "NAICS_2022_6_NAME"])
    .size()
    .reset_index(name="count")
)

top3 = top_industries.groupby("CLUSTER").apply(lambda x: x.nlargest(3, "count")).reset_index(drop=True)


pivot_df = top3.pivot(index="CLUSTER", columns="NAICS_2022_6_NAME", values="count").fillna(0)


pivot_df.plot(kind='bar', stacked=True, figsize=(10,6), colormap='tab20')  

plt.title("Top 3 Industries per Cluster")
plt.xlabel("Cluster")
plt.ylabel("Number of Job Postings")
plt.legend(title="Industry", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()



```

```{python}
#Predicting Salaries based on Location Data through Linear Regression
#*Decided to run it in Pandas with Scikit as it's already been converted and cleaned

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt

reg_data = clean_pdf[["SALARY", "STATE_NAME", "REMOTE_TYPE_NAME", "URBAN_RURAL"]]

X = pd.get_dummies(reg_data[["STATE_NAME", "REMOTE_TYPE_NAME", "URBAN_RURAL"]], drop_first=True)
y = reg_data["SALARY"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"Evaulation Metrics:")
print(f"RMSE: {rmse:,.2f}")
print(f"R2: {r2:.3f}")

df_coef = pd.DataFrame({
    "Feature": X.columns,
    "Coefficient": model.coef_
}).sort_values(by="Coefficient", ascending=False)

print("\nTop PositiveInfluences on Salary:")
print(df_coef.head(10))

print("\nTop Negative Influences on Salary:")
print(df_coef.tail(10))


plt.figure(figsize=(12,9))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel("Actual Salary")
plt.ylabel("Predicted Salary")
plt.title("Predicted vs Actual Salaries")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.tight_layout()
plt.show()


```